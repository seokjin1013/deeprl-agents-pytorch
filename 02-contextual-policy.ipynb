{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습을 구성하는 여러 요소 중 3가지는 아래와 같다.\n",
    "\n",
    "1. 액션 의존성- Agent가 하는 action에 따라 받는 reward가 다르다.\n",
    "\n",
    "2. 시간 의존성- action에 대한 reward를 바로바로 받는 것이 아니다. 얼마의 시간이 경과한 뒤 reward를 받을 수도 있고 sparse하다.\n",
    "\n",
    "3. 상태 의존성- Environment의 상태에 따라 받는 reward가 다르다.\n",
    "\n",
    "이번 예시는 bandit에 상태정보가 추가된 **컨텍스트 밴딧**문제로서, 2번만을 고려하지 않는다.\n",
    "\n",
    "아래는 강화학습 과정을 나타낸다.\n",
    "\n",
    "1. Agent는 Environment로부터 받은 상태를 기반으로 action을 생산한다. (단, 이번 Bandit 예제에서는 Environement의 상태는 없다.)\n",
    "\n",
    "2. Environment는 Agent로부터 받은 action을 기반으로 reward를 생산한다. 그리고 자신을 그 action을 기반으로 상태를 업데이트 한다. (단, Environment의 상태가 없기 때문에 생략한다.)\n",
    "\n",
    "3. Agent는 앞서 생산했던 action을 주고 어떤 reward를 받았는지를 통해 학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, bandit_arms):\n",
    "        self.bandit_arms = list(bandit_arms)\n",
    "        self.num_arms = len(bandit_arms)\n",
    "\n",
    "    def pull_bandit(self, action):\n",
    "        result = np.random.randn()\n",
    "        return 1 if result > self.bandit_arms[action] else -1\n",
    "\n",
    "bandits = [\n",
    "    Environment([0.2, 0, -0.2, -2]),\n",
    "    Environment([0.1, -5, 0, 2]),\n",
    "    Environment([-2, 1, -1, 0.25]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, num_actions, num_bandits):\n",
    "        self.num_bandits = num_bandits\n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_bandits, num_actions, False),\n",
    "            torch.nn.Softmax(dim=0),\n",
    "        )\n",
    "        torch.nn.init.ones_(self.policy[0].weight)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=1e-3)\n",
    "\n",
    "    def action(self, state:int) -> int:\n",
    "        with torch.inference_mode():\n",
    "            state = torch.tensor(state)\n",
    "            state = torch.nn.functional.one_hot(state, num_classes=self.num_bandits).float()\n",
    "            output = self.policy(state)\n",
    "            action = torch.multinomial(output, 1)\n",
    "        return action.detach().item()\n",
    "\n",
    "    def train(self, action, state, reward):\n",
    "        responsible_output = self.policy[0].weight[action, state]\n",
    "        loss = -reward * torch.log(responsible_output)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "agent = Agent(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 [10.   4.   7.5]\n",
      "1000 [24.   5.5 20. ]\n",
      "1500 [34.25 11.75 34.5 ]\n",
      "2000 [47.5 19.5 56. ]\n",
      "2500 [64.25 33.25 76.  ]\n",
      "3000 [83.  50.5 95. ]\n",
      "3500 [107.75  69.   115.75]\n",
      "4000 [128.25  96.5  143.75]\n",
      "4500 [148.   120.25 168.75]\n",
      "5000 [170.75 148.   202.75]\n",
      "5500 [195.75 181.25 236.  ]\n",
      "6000 [219.25 215.25 265.5 ]\n",
      "6500 [251.75 245.5  297.25]\n",
      "7000 [281.5 281.  330. ]\n",
      "7500 [317.25 313.   365.25]\n",
      "8000 [352.5 351.  397. ]\n",
      "8500 [384.   393.75 428.25]\n",
      "9000 [418.   429.25 459.75]\n",
      "9500 [448.  473.5 501. ]\n",
      "10000 [488.75 507.5  540.75]\n"
     ]
    }
   ],
   "source": [
    "total_episode = 10000\n",
    "total_reward = np.zeros((4, 3))\n",
    "\n",
    "for i in range(total_episode):\n",
    "    bandit_index = np.random.randint(3)\n",
    "    bandit = bandits[bandit_index]\n",
    "    action = agent.action(bandit_index)\n",
    "    reward = bandit.pull_bandit(action)\n",
    "    agent.train(action, bandit_index, reward)\n",
    "    total_reward[action, bandit_index] += reward\n",
    "    if i % 500 == 499:\n",
    "        print(i + 1, total_reward.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.7053, 0.9026, 4.6664],\n",
       "        [1.0883, 5.2763, 0.0273],\n",
       "        [1.3529, 0.9433, 2.8910],\n",
       "        [4.8534, 0.0183, 0.7820]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy[0].weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
